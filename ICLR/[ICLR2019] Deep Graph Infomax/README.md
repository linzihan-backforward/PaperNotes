# Deep Graph Infomax

------

## Motivation

​	如何对图结构的数据进行表示是现在图问题的一个热点，现今取得非常好效果的GCN是一个有监督的方法，然而，现实中大量的图数据是没有标签信息的，所以使用图数据进行无监督的表示同样非常重要，已有的无监督的方法大多是基于随机游走的，他们的假设是在图中距离近的节点应当在表示空间中同样相近，然而这种方法也存在一定的局限：第一，这种方法过分强调了距离的影响而忽视了网络结构的作用。第二，这种方法的表现高度依赖于超参数的选择。第三，随着基于GCN的更强的模型的出现，其所基于的临近相似假设还是否成立不再那么清晰。为了解决上面提到的问题，一种新的无监督方法被提出，它基于最大相互信息，将之前在图像领域使用的Deep InfoMax方法进行了迁移，在图数据上得到了比其他的有监督和无监督方法更加出色的效果。

## Model

​	首先，我们的模型的目标是完成一个图中节点的嵌入，即给定一个特征矩阵X，其中每一行为一个节点i的特征向量，和整个图的邻接矩阵A，我们需要学到一个嵌入函数e（X，A）=H，其中H的每一行为一个图中节点的表示。在GCN的方法中这里的e使用的是多层卷积的堆叠，最后使用带监督信息的cross entropy作为目标函数，但是我们的目标是无监督，所以这里的e函数的结构依旧可以使用GCN的结构，可是最后的目标函数需要重新设计，那么怎么设计呢？就需要用到相互信息的概念了。

​	前面说到，之前的基于随机游走的无监督图嵌入过分强调一个节点周围的邻居而对整个图的结构建模不够，为了克服这一点，我们另外引入一个s向量作为整个图的总结向量，包含全图的信息，对于图中的每一个节点i其表示hi与所属图的总结向量s应当具有最大的互信息，这便是本模型所基于的假设。具体地，对于一个图有总结函数R（e（X，A））= s ，即得到每一个节点的嵌入之后将所有嵌入送入一个总结函数得到图的总结向量s。有了图的总结向量怎么计算互信息呢？我们定义一个打分函数D，D（hi，s）=score ，这样我们便计算出了每一个节点与图的互信息，接下来使用上面定义的函数来组合为最终的目标函数。

​	回想skip-gram模型中目标函数所采用的最大值最小值比对的方法，我们可以使用这种基于负采样的目标函数，即最大化正样本的互信息而最小化负样本的互信息。有了一个图之后，图中的节点都是正样本，那么负样本从哪里来呢？加入我们有多个图，那么自然将另外一个完全不同的图拿过来作为负样本即可，那只有一个图怎么办呢？这时我们就需要一个采样函数C（X，A）=（X^，A^）这样得到的（X^，A^），我们把它当作另外一个图，即负样本，在下面的实验部分，我们可以看到其实函数C的定义非常简单。这样我们的目标函数就可以定义如下：

![image-20200107143251709](https://github.com/linzihan-backforward/PaperNotes/blob/master/ICLR/%5BICLR2019%5D%20Deep%20Graph%20Infomax/image-20200107143251709.png?raw=true)

​	这种方法其实就是最大化正负样本之间的J-S散度，论文中还有一节理论推导，本文以介绍方法为主，所以省略。

​	整个DGI的模型计算过程可以用一个例子来概括：

![image-20200107143756548](https://github.com/linzihan-backforward/PaperNotes/blob/master/ICLR/%5BICLR2019%5D%20Deep%20Graph%20Infomax/image-20200107143756548.png?raw=true)

## Experiments

​	模型部分就完了？好像有点晕？没关系，在实验部分我们结合具体地任务再将模型进行细化，给出其中各个函数的具体定义。

### Datasets

​	这里所使用的数据集都是图领域非常常用的数据集，分为了三类

- 学术引用网络小数据集：Cora，Citeseer，Pubmed
- 社交网络大数据集：Reddit
- 多图蛋白质分类数据集：PPI

### Setup

​	下面我们以学术引用数据集来详细完善模型中的函数定义，在其他数据集中可能有其他的定义方式，具体可以参见原论文。

​	首先，第一个需要定义的函数e，上面也提到沿用GCN的思想，这里的e使用的就是一个单层的GCN模型，使用下面的公式计算；![image-20200107144425243](https://github.com/linzihan-backforward/PaperNotes/blob/master/ICLR/%5BICLR2019%5D%20Deep%20Graph%20Infomax/image-20200107144425243.png?raw=true)

A^为添加了自环的邻接矩阵，D^为A^相应的度矩阵，Θ为本层的参数，熟悉GCN的同学应该不用过多解释。

​	第二个需要详细定义的是负采样需要用到的采样函数C，为了让表示能够合理地编码图中的结构信息，我们将图的连接关系A原样保留，即A^ = A ，而每一个节点的的特征向量随机打乱，即将X的行进行shuffle得到X^，这样我们相当于保留了图的所有节点，但是打乱了每个结点的位置来得到一个新图作为负样本。

​	接下来是总结函数R，得到了H之后，我们定义总结函数是对于所有节点进行简单的平均操作，实验证明这样已经可以取得相当好的结果，但是随着图的增大，其他复杂的函数获取会更有效。

![image-20200107145325481](https://github.com/linzihan-backforward/PaperNotes/blob/master/ICLR/%5BICLR2019%5D%20Deep%20Graph%20Infomax/image-20200107145325481.png?raw=true)

​	最后一项，打分函数，对于两个向量，求它们的得分，双线性映射是一个常用的方法。

![image-20200107145533266](https://github.com/linzihan-backforward/PaperNotes/blob/master/ICLR/%5BICLR2019%5D%20Deep%20Graph%20Infomax/image-20200107145533266.png?raw=true)

​	好了，现在模型中用到的的所有函数都有了具体地定义，当然面对不同的数据集时最优的选择可能不同，总结一下，当前整个模型中包含的参数其实非常少，嵌入函数中的Θ、打分函数中的W，只需要这两个我们就可以把原本的有监督的GCN模型迁移到使用互信息来进行无监督的训练上来，整体的模型上还是不复杂的。

​	本模型作者开源了实现代码：https://github.com/PetarV-/DGI

### baselines

​	DeepWalk、GCN、LP、Planetoid、raw_feature、GraphSAGE、FastGCN等

<img src="https://github.com/linzihan-backforward/PaperNotes/blob/master/ICLR/%5BICLR2019%5D%20Deep%20Graph%20Infomax/image-20200107151158883.png?raw=true" alt="image-20200107151158883" style="zoom:67%;" />

## 个人见解

​	这篇文章我认为还是质量比较高，创新性比较强的文章之一，同时也可以算是开创了一种新的与randomWalk相对应的图数据无监督学习的方法，同时其模型的整个结构很清晰，核心目标就是把互信息这个概念迁移到图上来，在算法设计上充分进行了抽象，每一步的函数具体实现都不固定，给了模型很好的理论性和拓展性，但是这也是一个双刃剑，在模型设计部分频繁的定义抽象函数而在实验部分只是简单的进行选择，给人理解上缺乏一种选择的合理性和可解释性，如文中选择负采样函数C时并没有讲明负样本所充当的作用和应具有的特点，直接三言两语说完C定义，给人一种生拉硬拽的牵强感。这篇文章作为在randomWalk5年之后的对等模型可能又会一批基于这种方法的图上的模型，我觉得一个可以思考的未来工作是将这种互信息与randomWalk相结合，二者取长补短，互信息专注于整体的结构，randomWalk专注于小邻域的距离，可能是一个很好的点。